{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5053ac9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "RANDOM_SEED = 42\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score as auroc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from pytorch_lightning.utilities.warnings import LightningDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=LightningDeprecationWarning)\n",
    "import re\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99f2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"demo_50000.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a669f",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d5e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s):\n",
    "    regs = [\n",
    "        r'(\\/\\/www[^\\s]+)',\n",
    "        r'(pic.twitter.com\\/[^\\s]+)',\n",
    "        r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)',\n",
    "        r'(?i)\\b((?:https?:\\/\\/[^\\s]+))',\n",
    "        r'https',\n",
    "    ]\n",
    "    \n",
    "    cleaned_s=s.replace(\"RT\", \"\")\n",
    "    cleaned_s=cleaned_s.replace(\"&amp\", \"\")\n",
    "    cleaned_s=cleaned_s.replace(\"\\\\n\", \"\")\n",
    "    cleaned_s = \" \".join([x[:-2] for x in cleaned_s.split(\"\\\\x\")])\n",
    "    cleaned_s=cleaned_s.lower()[1:]\n",
    "    \n",
    "    for reg in regs:\n",
    "        cleaned_s = re.sub(reg, \" \", cleaned_s)\n",
    "    return cleaned_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8c770d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "      <th>retweet</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>443098</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"If Pres. Obama is serious about lifting the ...</td>\n",
       "      <td>ObamaAboutFace RequireAPlan</td>\n",
       "      <td>5</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "      <td>if pres  obama is serious about lifting the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136957</th>\n",
       "      <td>27</td>\n",
       "      <td>b'Wishing all those celebrating a happy #Easte...</td>\n",
       "      <td>Easter</td>\n",
       "      <td>7</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "      <td>False</td>\n",
       "      <td>wishing all those celebrating a happy  easter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459257</th>\n",
       "      <td>0</td>\n",
       "      <td>b'UAM College of Technology in Crossett is hom...</td>\n",
       "      <td>ar4</td>\n",
       "      <td>2</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "      <td>uam college of technology in crossett is home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29136</th>\n",
       "      <td>1695</td>\n",
       "      <td>b'Helping #PuertoRico should be our primary co...</td>\n",
       "      <td>PuertoRico NotOnePenny</td>\n",
       "      <td>732</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "      <td>False</td>\n",
       "      <td>helping  puertorico should be our primary con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39352</th>\n",
       "      <td>22</td>\n",
       "      <td>b\"It\\xe2\\x80\\x99s crazy that we have Ohioans w...</td>\n",
       "      <td>POWADA</td>\n",
       "      <td>4</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "      <td>99s crazy that we have ohioans willing abl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351442</th>\n",
       "      <td>1</td>\n",
       "      <td>b'#Cures is a landmark medical innovation pack...</td>\n",
       "      <td>Cures</td>\n",
       "      <td>2</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "      <td>cures is a landmark medical innovation packa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274584</th>\n",
       "      <td>2</td>\n",
       "      <td>b'Happy 40th birthday to DFW airport! Honored ...</td>\n",
       "      <td>DFW40</td>\n",
       "      <td>5</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "      <td>happy 40th birthday to dfw airport  honored t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38412</th>\n",
       "      <td>168</td>\n",
       "      <td>b'Reignite Cold War with Russia after email le...</td>\n",
       "      <td>foreignpolicy fail</td>\n",
       "      <td>102</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "      <td>reignite cold war with russia after email lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315203</th>\n",
       "      <td>33</td>\n",
       "      <td>b'It was the people who defeated #Trumpcare! W...</td>\n",
       "      <td>Trumpcare BrooklynResists</td>\n",
       "      <td>12</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "      <td>False</td>\n",
       "      <td>it was the people who defeated  trumpcare  we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351329</th>\n",
       "      <td>1</td>\n",
       "      <td>b'Great news from @DeptVetAffairs and @HUDgov ...</td>\n",
       "      <td>homelessness Vets</td>\n",
       "      <td>2</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>D</td>\n",
       "      <td>False</td>\n",
       "      <td>great news from   and   that  homelessness am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        favorite_count                                          full_text  \\\n",
       "443098               0  b\"If Pres. Obama is serious about lifting the ...   \n",
       "136957              27  b'Wishing all those celebrating a happy #Easte...   \n",
       "459257               0  b'UAM College of Technology in Crossett is hom...   \n",
       "29136             1695  b'Helping #PuertoRico should be our primary co...   \n",
       "39352               22  b\"It\\xe2\\x80\\x99s crazy that we have Ohioans w...   \n",
       "...                ...                                                ...   \n",
       "351442               1  b'#Cures is a landmark medical innovation pack...   \n",
       "274584               2  b'Happy 40th birthday to DFW airport! Honored ...   \n",
       "38412              168  b'Reignite Cold War with Russia after email le...   \n",
       "315203              33  b'It was the people who defeated #Trumpcare! W...   \n",
       "351329               1  b'Great news from @DeptVetAffairs and @HUDgov ...   \n",
       "\n",
       "                           hashtags  retweet_count    year party_id  retweet  \\\n",
       "443098  ObamaAboutFace RequireAPlan              5  2013.0        R    False   \n",
       "136957                       Easter              7  2018.0        D    False   \n",
       "459257                          ar4              2  2018.0        R    False   \n",
       "29136        PuertoRico NotOnePenny            732  2017.0        D    False   \n",
       "39352                        POWADA              4  2020.0        R    False   \n",
       "...                             ...            ...     ...      ...      ...   \n",
       "351442                        Cures              2  2016.0        R    False   \n",
       "274584                        DFW40              5  2014.0        R    False   \n",
       "38412            foreignpolicy fail            102  2016.0        R    False   \n",
       "315203    Trumpcare BrooklynResists             12  2017.0        D    False   \n",
       "351329            homelessness Vets              2  2013.0        D    False   \n",
       "\n",
       "                                             cleaned_text  \n",
       "443098   if pres  obama is serious about lifting the s...  \n",
       "136957   wishing all those celebrating a happy  easter...  \n",
       "459257   uam college of technology in crossett is home...  \n",
       "29136    helping  puertorico should be our primary con...  \n",
       "39352       99s crazy that we have ohioans willing abl...  \n",
       "...                                                   ...  \n",
       "351442    cures is a landmark medical innovation packa...  \n",
       "274584   happy 40th birthday to dfw airport  honored t...  \n",
       "38412    reignite cold war with russia after email lea...  \n",
       "315203   it was the people who defeated  trumpcare  we...  \n",
       "351329   great news from   and   that  homelessness am...  \n",
       "\n",
       "[50000 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"retweet\"] = data[\"full_text\"].str.slice(2,4)==\"RT\"\n",
    "data[\"cleaned_text\"]=data[\"full_text\"].apply(clean_str)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4285d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(\" \", inplace=True)\n",
    "data[\"party_R\"]=data[\"party_id\"].apply(lambda x: 0 if x==\"D\" else 1)\n",
    "data[\"party_D\"]=data[\"party_id\"].apply(lambda x: 0 if x==\"R\" else 1)\n",
    "train_df = data.sample(frac=0.8)\n",
    "val_df = data.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b8e8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.read_csv(\"test.csv\")\n",
    "predict_df = predict_df.fillna(\"\")\n",
    "predict_df[\"retweet\"] = predict_df[\"full_text\"].str.slice(2,4)==\"RT\"\n",
    "predict_df[\"cleaned_text\"]=predict_df[\"full_text\"].apply(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2f1ae",
   "metadata": {},
   "source": [
    "# Network Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14451120",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421aca1",
   "metadata": {},
   "source": [
    "## Create Dataset for BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab8478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: pd.DataFrame,\n",
    "                 tokenizer: BertTokenizer,\n",
    "                 max_token_len: int = 100\n",
    "                ):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row.cleaned_text\n",
    "        label1, label2 = 0, 0\n",
    "        if \"party_id\" in data_row:\n",
    "            label1 = data_row.party_R\n",
    "            label2 = data_row.party_D\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=self.max_token_len,\n",
    "                        return_token_type_ids=False,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                    )\n",
    "        \n",
    "\n",
    "        return dict(\n",
    "            text=text,\n",
    "            input_ids=encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            \n",
    "            labels=torch.FloatTensor(np.array([label1, label2]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5a2ec6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TwitterDataset(data, tokenizer, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c978c2",
   "metadata": {},
   "source": [
    "## Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d30a644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4c6d5",
   "metadata": {},
   "source": [
    "## Create DataModule (Combination of DataLoaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa2846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, val_df, predict_df, tokenizer, batch_size=50, max_token_len=100):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.predict_df = predict_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = TwitterDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.val_dataset = TwitterDataset(\n",
    "            self.val_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d155fd",
   "metadata": {},
   "source": [
    "# Hyper parameters for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20330091",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 30\n",
    "data_module = TwitterDataModule(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    predict_df,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212725e",
   "metadata": {},
   "source": [
    "# Define Training/Testing/Validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6caa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterTagger(pl.LightningModule):\n",
    "    def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            BERT_MODEL_NAME, return_dict=True)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            TwitterDataset(\n",
    "                predict_df,\n",
    "                tokenizer,\n",
    "                100\n",
    "            ),\n",
    "            batch_size = 50\n",
    "        )\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        _, preds = self(input_ids, attention_mask,)\n",
    "        return preds.cpu().detach().numpy()\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        \n",
    "        for i, name in enumerate([\"party_R\", \"party_D\"]):\n",
    "            class_roc_auc = auroc(labels[:, i].numpy(), (predictions[:, i].numpy()>0.5).astype(int))\n",
    "            self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca106877",
   "metadata": {},
   "source": [
    "# Calculate other numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ef9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd7be8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1333, 6665)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de123389",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7f3ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TwitterTagger(\n",
    "  n_classes=2,\n",
    "  n_warmup_steps=warmup_steps,\n",
    "  n_training_steps=total_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27825f",
   "metadata": {},
   "source": [
    "## Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae97e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=\"checkpoints\",\n",
    "  filename=\"best-checkpoint\",\n",
    "  save_top_k=2,\n",
    "  verbose=True,\n",
    "  monitor=\"val_loss\",\n",
    "  mode=\"min\"\n",
    ")\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f2accc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"Twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05104ce6",
   "metadata": {},
   "source": [
    "# Initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f8a60f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\miniconda\\envs\\Kaggle\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=[<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x000002102300E7F0>, <pytorch_lightning.callbacks.early_stopping.EarlyStopping object at 0x000002102300E580>])` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=[<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x000002102300E7F0>, <pytorch_lightning.callbacks.early_stopping.EarlyStopping object at 0x000002102300E580>])`.\n",
      "  rank_zero_deprecation(\n",
      "C:\\miniconda\\envs\\Kaggle\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "  logger=logger,\n",
    "  checkpoint_callback=[checkpoint_callback,early_stopping_callback],\n",
    "  max_epochs=N_EPOCHS,\n",
    "  gpus=1,\n",
    "  progress_bar_refresh_rate=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234d4c0",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19cb0ad5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\miniconda\\envs\\Kaggle\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | bert       | BertModel | 108 M \n",
      "1 | classifier | Linear    | 1.5 K \n",
      "2 | criterion  | BCELoss   | 0     \n",
      "-----------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "433.247   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdea89d3f1b40c1b33ee2739309eb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696583bb",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fea84f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lightning_logs\\\\Twitter\\\\version_16\\\\checkpoints\\\\epoch=4-step=6670.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ce23ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trained_model = TwitterTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fd26164",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82395118",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at lightning_logs\\Twitter\\version_16\\checkpoints\\epoch=4-step=6670.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at lightning_logs\\Twitter\\version_16\\checkpoints\\epoch=4-step=6670.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cb549e325e49778efbe4bef0639427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1334it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.predict()\n",
    "result = np.concatenate(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e795cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [\"R\" if x else \"D\" for x in result[:,0]>result[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80f33114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"party\":result}).reset_index().rename(columns={\"index\":\"id\"}).to_csv(\"submission_bert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a185eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
